{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        #print(dirname)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-11T16:11:15.098046Z","iopub.execute_input":"2022-07-11T16:11:15.098468Z","iopub.status.idle":"2022-07-11T16:11:15.135949Z","shell.execute_reply.started":"2022-07-11T16:11:15.098383Z","shell.execute_reply":"2022-07-11T16:11:15.134471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\n# You'll generate plots of attention in order to see which parts of an image\n# our model focuses on during captioning\nimport matplotlib.pyplot as plt\n\n# Scikit-learn includes many helpful utilities\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nimport re\nimport numpy as np\nimport os\nimport time\nimport json\nfrom glob import glob\nfrom PIL import Image\nimport pickle\nfrom tqdm.auto import tqdm\nimport csv","metadata":{"execution":{"iopub.status.busy":"2022-07-11T16:11:24.079319Z","iopub.execute_input":"2022-07-11T16:11:24.080177Z","iopub.status.idle":"2022-07-11T16:11:30.800807Z","shell.execute_reply.started":"2022-07-11T16:11:24.080136Z","shell.execute_reply":"2022-07-11T16:11:30.799916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import tensorflow as tf\n\n# # You'll generate plots of attention in order to see which parts of an image\n# # your model focuses on during captioning\n# import matplotlib.pyplot as plt\n\n# import collections\n# import random\n# import numpy as np\n# import os\n# import time\n# import json\n# from PIL import Image\n\n\n# #Download caption annotation files\n# annotation_folder = '/annotations/'\n# if not os.path.exists(os.path.abspath('.') + annotation_folder):\n#   annotation_zip = tf.keras.utils.get_file('captions.zip',\n#                                            cache_subdir=os.path.abspath('.'),\n#                                            origin='http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n#                                            extract=True)\n#   annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n#   os.remove(annotation_zip)\n\n# # Download image files\n# image_folder = '/train2014/'\n# if not os.path.exists(os.path.abspath('.') + image_folder):\n#   image_zip = tf.keras.utils.get_file('train2014.zip',\n#                                       cache_subdir=os.path.abspath('.'),\n#                                       origin='http://images.cocodataset.org/zips/train2014.zip',\n#                                       extract=True)\n#   PATH = os.path.dirname(image_zip) + image_folder\n#   os.remove(image_zip)\n# else:\n#   PATH = os.path.abspath('.') + image_folder\n","metadata":{"execution":{"iopub.status.busy":"2022-07-11T16:11:37.706568Z","iopub.execute_input":"2022-07-11T16:11:37.707231Z","iopub.status.idle":"2022-07-11T16:11:37.712602Z","shell.execute_reply.started":"2022-07-11T16:11:37.707191Z","shell.execute_reply":"2022-07-11T16:11:37.711415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Visualize dataset'''\n\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n\n#Loading captions\ntrain = json.load(open('../input/ms-coco-dataset/captions/annotations/captions_train2014.json', 'r'))\nval = json.load(open('../input/ms-coco-dataset/captions/annotations/captions_val2014.json', 'r'))\n\n#Annotations keys\nprint(\"\\nInside annotations\")\nprint(train.keys())\nprint(val.keys())\n\n#Annotations info\nprint(\"\\n\\nAnnotations information\")\nprint(train['info'])\nprint(val['info'])\n\n#Dataset size\nprint(\"\\n\\nSize of dataset using\")\nprint(\"Number of training images is \"+str(len(train['images'])))\nprint(\"Number of captions in training set\"+str(len(train['annotations'])))\nprint(\"Number of validation set images\"+str(len(val['images'])))\nprint(\"Number of vaptions in validation set\"+str(len(val['annotations'])))\n\n# combine all images and annotations together\nimgs = train['images']\nannots = train['annotations']\n\n# Group annotations by image\nitoa = {}\nfor a in annots:\n    imgid = a['image_id']\n    if imgid not in itoa:\n        itoa[imgid] = []\n    itoa[imgid].append(a)\n\n# Create the json blob for an image and its captions\nout = []\nfor i, img in enumerate(imgs):\n    imgid = img['id']\n    \n    loc = 'train' if 'train' in img['file_name'] else 'val'\n    loc = '../input/ms-coco-dataset/train2014/train2014'\n    jimg = {}\n    jimg['file_path'] = os.path.join(loc, img['file_name'])\n    jimg['id'] = imgid\n    jimg['captions'] = [a['caption'] for a in itoa[imgid]]\n    \n    out.append(jimg)\n    \n#Dumping in json format for better understanding\njson.dump(out, open('coco_raw.json', 'w'))\n\n\n#Display first image and it's captions\nprint(\"\\n\\nDetails of first image in training set\")\nfirst_img = out[0]\nprint(first_img)\n\nimage = mpimg.imread(first_img['file_path'])\nprint(image.shape)\nplt.imshow(image)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-11T16:11:37.714420Z","iopub.execute_input":"2022-07-11T16:11:37.715031Z","iopub.status.idle":"2022-07-11T16:11:42.786930Z","shell.execute_reply.started":"2022-07-11T16:11:37.714977Z","shell.execute_reply":"2022-07-11T16:11:42.785204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Shuffling images and captions'''\n\nPATH = '../input/ms-coco-dataset/train2014/train2014/'\nall_captions = []\nall_img_name_vector = []\nfor a in annots:\n    caption = '<start> ' + a['caption'] + ' <end>'\n    image_id = a['image_id']\n    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n\n    all_img_name_vector.append(full_coco_image_path)\n    all_captions.append(caption)\n    \ntrain_captions, img_name_vector = shuffle(all_captions,\n                                          all_img_name_vector,\n                                          random_state=1)\n\n# Select the first 30000 captions from the shuffled set\nnum_examples = 30000\ntrain_captions = train_captions[:num_examples]\nimg_name_vector = img_name_vector[:num_examples]\n","metadata":{"execution":{"iopub.status.busy":"2022-07-11T16:11:42.788473Z","iopub.execute_input":"2022-07-11T16:11:42.789616Z","iopub.status.idle":"2022-07-11T16:11:43.818961Z","shell.execute_reply.started":"2022-07-11T16:11:42.789578Z","shell.execute_reply":"2022-07-11T16:11:43.816561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_captions), len(all_captions)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T16:11:43.821119Z","iopub.execute_input":"2022-07-11T16:11:43.822105Z","iopub.status.idle":"2022-07-11T16:11:43.829529Z","shell.execute_reply.started":"2022-07-11T16:11:43.821997Z","shell.execute_reply":"2022-07-11T16:11:43.828672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Preprocess images for inceptionv3'''\ndef load_image(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, (299, 299))\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n    return img, image_path\n\nimage_model = tf.keras.applications.InceptionV3(include_top=False,\n                                                weights='imagenet')\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output\n\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n\n# Get unique images\nencode_train = sorted(set(img_name_vector))\n\n#Feeding image dataset\nimage_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\nimage_dataset = image_dataset.map(\n  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(32)\n\n#os.remove('../working/Image')\nos.mkdir(\"Image\")\nOUTPUT_PATH = \"../working/Image/\"\nfor img, path in tqdm(image_dataset):\n  batch_features = image_features_extract_model(img)\n  batch_features = tf.reshape(batch_features,\n                              (batch_features.shape[0], -1, batch_features.shape[3]))\n\n  for bf, p in zip(batch_features, path):\n    file_name = p.numpy().decode(\"utf-8\").split('/')[-1]\n    np.save(OUTPUT_PATH + file_name, bf.numpy())\n","metadata":{"execution":{"iopub.status.busy":"2022-07-11T16:11:43.831621Z","iopub.execute_input":"2022-07-11T16:11:43.832441Z","iopub.status.idle":"2022-07-11T16:15:41.964879Z","shell.execute_reply.started":"2022-07-11T16:11:43.832397Z","shell.execute_reply":"2022-07-11T16:15:41.963572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Preprocess captions'''\n\n#Find max length of any caption in the dataset\ndef calc_max_length(tensor):\n    return max(len(t) for t in tensor)\n\n# Choose the top 5000 words from the vocabulary\ntop_k = 5000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n                                                  oov_token=\"<unk>\",\n                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\ntokenizer.fit_on_texts(train_captions)\ntrain_seqs = tokenizer.texts_to_sequences(train_captions)\n\n#Setting word index of pad\ntokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'\n\n# Create the tokenized vectors\ntrain_seqs = tokenizer.texts_to_sequences(train_captions)\n\n# Pad each vector to the max_length of the captions\ncap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n\n# Calculates the max_length, which is used to store the attention weights\nmax_length = calc_max_length(train_seqs)\nprint(max_length)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T16:15:41.966197Z","iopub.execute_input":"2022-07-11T16:15:41.966658Z","iopub.status.idle":"2022-07-11T16:15:43.458806Z","shell.execute_reply.started":"2022-07-11T16:15:41.966621Z","shell.execute_reply":"2022-07-11T16:15:43.457663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Split train-test and create tf dataset'''\n\n# Create training and validation sets using an 80-20 split\nimg_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n                                                                    cap_vector,\n                                                                    test_size=0.0333,\n                                                                    random_state=0)\n\n#Other parameters\nBATCH_SIZE = 64\nBUFFER_SIZE = 1000\nembedding_dim = 256\nunits = 512\nvocab_size = top_k + 1\nnum_steps = len(img_name_train) // BATCH_SIZE\n# Shape of the vector extracted from InceptionV3 is (64, 2048)\n# These two variables represent that vector shape\nfeatures_shape = 2048\nattention_features_shape = 64\n\n# Load the numpy files\ndef map_func(img_name, cap):\n  filename = OUTPUT_PATH + img_name.decode('utf-8').split('/')[-1] + \".npy\"\n  img_tensor = np.load(filename)\n  return img_tensor, cap\n\n#Creating tensor dataset\ndataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n\n# Use map to load the numpy files in parallel\ndataset = dataset.map(lambda item1, item2: tf.numpy_function(\n          map_func, [item1, item2], [tf.float32, tf.int32]),\n          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n# Shuffle and batch\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\ndataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T16:15:43.460099Z","iopub.execute_input":"2022-07-11T16:15:43.460797Z","iopub.status.idle":"2022-07-11T16:15:43.641725Z","shell.execute_reply.started":"2022-07-11T16:15:43.460758Z","shell.execute_reply":"2022-07-11T16:15:43.640646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''MODEL'''\n\nclass BahdanauAttention(tf.keras.Model):\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.V = tf.keras.layers.Dense(1)\n\n  def call(self, features, hidden):\n    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n\n    # hidden shape == (batch_size, hidden_size)\n    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n\n    # score shape == (batch_size, 64, hidden_size)\n    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n\n    # attention_weights shape == (batch_size, 64, 1)\n    # you get 1 at the last axis because you are applying score to self.V\n    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n\n    # context_vector shape after sum == (batch_size, hidden_size)\n    context_vector = attention_weights * features\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n\n    return context_vector, attention_weights\n\nclass CNN_Encoder(tf.keras.Model):\n    # Since you have already extracted the features and dumped it using pickle\n    # This encoder passes those features through a Fully connected layer\n    def __init__(self, embedding_dim):\n        super(CNN_Encoder, self).__init__()\n        # shape after fc == (batch_size, 64, embedding_dim)\n        self.fc = tf.keras.layers.Dense(embedding_dim)\n\n    def call(self, x):\n        x = self.fc(x)\n        x = tf.nn.relu(x)\n        return x\n    \nclass RNN_Decoder(tf.keras.Model):\n  def __init__(self, embedding_dim, units, vocab_size):\n    super(RNN_Decoder, self).__init__()\n    self.units = units\n\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    self.fc1 = tf.keras.layers.Dense(self.units)\n    self.fc2 = tf.keras.layers.Dense(vocab_size)\n\n    self.attention = BahdanauAttention(self.units)\n\n  def call(self, x, features, hidden):\n    # defining attention as a separate model\n    context_vector, attention_weights = self.attention(features, hidden)\n\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n    x = self.embedding(x)\n\n    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n    # passing the concatenated vector to the GRU\n    output, state = self.gru(x)\n\n    # shape == (batch_size, max_length, hidden_size)\n    x = self.fc1(output)\n\n    # x shape == (batch_size * max_length, hidden_size)\n    x = tf.reshape(x, (-1, x.shape[2]))\n\n    # output shape == (batch_size * max_length, vocab)\n    x = self.fc2(x)\n\n    return x, state, attention_weights\n\n  def reset_state(self, batch_size):\n    return tf.zeros((batch_size, self.units))\n\nencoder = CNN_Encoder(embedding_dim)\ndecoder = RNN_Decoder(embedding_dim, units, vocab_size)\n\noptimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n\n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask\n\n  return tf.reduce_mean(loss_)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T16:15:43.643212Z","iopub.execute_input":"2022-07-11T16:15:43.643631Z","iopub.status.idle":"2022-07-11T16:15:44.005743Z","shell.execute_reply.started":"2022-07-11T16:15:43.643588Z","shell.execute_reply":"2022-07-11T16:15:44.004828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Checkpoint manager'''\n\ncheckpoint_path = \"../working/checkpoints/train\"\nckpt = tf.train.Checkpoint(encoder=encoder,\n                           decoder=decoder,\n                           optimizer = optimizer)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\nstart_epoch = 0\nif ckpt_manager.latest_checkpoint:\n  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n  # restoring the latest checkpoint in checkpoint_path\n  ckpt.restore(ckpt_manager.latest_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T16:15:44.007626Z","iopub.execute_input":"2022-07-11T16:15:44.008002Z","iopub.status.idle":"2022-07-11T16:15:44.020268Z","shell.execute_reply.started":"2022-07-11T16:15:44.007966Z","shell.execute_reply":"2022-07-11T16:15:44.019505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Keeping loss plot in separate cell as train step is called many times will reset loss plot\nloss_plot = []","metadata":{"execution":{"iopub.status.busy":"2022-07-11T16:15:44.021532Z","iopub.execute_input":"2022-07-11T16:15:44.022279Z","iopub.status.idle":"2022-07-11T16:15:44.026733Z","shell.execute_reply.started":"2022-07-11T16:15:44.022242Z","shell.execute_reply":"2022-07-11T16:15:44.025980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Training'''\n\n#Tensorflow train step function\n@tf.function\ndef train_step(img_tensor, target):\n  loss = 0\n\n  # initializing the hidden state for each batch\n  # because the captions are not related from image to image\n  hidden = decoder.reset_state(batch_size=target.shape[0])\n\n  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n\n  with tf.GradientTape() as tape:\n      features = encoder(img_tensor)\n\n      for i in range(1, target.shape[1]):\n          # passing the features through the decoder\n          predictions, hidden, _ = decoder(dec_input, features, hidden)\n\n          loss += loss_function(target[:, i], predictions)\n\n          # using teacher forcing\n          dec_input = tf.expand_dims(target[:, i], 1)\n\n  total_loss = (loss / int(target.shape[1]))\n\n  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n\n  gradients = tape.gradient(loss, trainable_variables)\n\n  optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n  return loss, total_loss\n\n","metadata":{"execution":{"iopub.status.busy":"2022-07-11T16:15:44.029776Z","iopub.execute_input":"2022-07-11T16:15:44.030319Z","iopub.status.idle":"2022-07-11T16:15:44.044540Z","shell.execute_reply.started":"2022-07-11T16:15:44.030278Z","shell.execute_reply":"2022-07-11T16:15:44.043541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 20\nfor epoch in range(start_epoch, EPOCHS):\n    start = time.time()\n    total_loss = 0\n\n    for (batch, (img_tensor, target)) in enumerate(dataset):\n        batch_loss, t_loss = train_step(img_tensor, target)\n        total_loss += t_loss\n\n        if batch % 100 == 0:\n            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n    # storing the epoch end loss value to plot later\n    loss_plot.append(total_loss / num_steps)\n\n    if epoch % 5 == 0:\n      ckpt_manager.save()\n\n    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n                                         total_loss/num_steps))\n    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","metadata":{"execution":{"iopub.status.busy":"2022-07-11T16:15:44.045807Z","iopub.execute_input":"2022-07-11T16:15:44.046611Z","iopub.status.idle":"2022-07-11T17:34:31.693357Z","shell.execute_reply.started":"2022-07-11T16:15:44.046570Z","shell.execute_reply":"2022-07-11T17:34:31.690177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Visualize training'''\n\nplt.plot(loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-11T17:34:31.698319Z","iopub.execute_input":"2022-07-11T17:34:31.699622Z","iopub.status.idle":"2022-07-11T17:34:32.001258Z","shell.execute_reply.started":"2022-07-11T17:34:31.699581Z","shell.execute_reply":"2022-07-11T17:34:32.000316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Caption\ndef evaluate(image):\n    attention_plot = np.zeros((max_length, attention_features_shape))\n\n    hidden = decoder.reset_state(batch_size=1)\n\n    temp_input = tf.expand_dims(load_image(image)[0], 0)\n    img_tensor_val = image_features_extract_model(temp_input)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n                                                 -1,\n                                                 img_tensor_val.shape[3]))\n\n    features = encoder(img_tensor_val)\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n    result = []\n\n    for i in range(max_length):\n        predictions, hidden, attention_weights = decoder(dec_input,\n                                                         features,\n                                                         hidden)\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n\n        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n        predicted_word = tf.compat.as_text(tokenizer.index_word[predicted_id])\n        result.append(predicted_word)\n\n        if predicted_word == '<end>':\n            return result, attention_plot\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot","metadata":{"execution":{"iopub.status.busy":"2022-07-11T17:34:32.002570Z","iopub.execute_input":"2022-07-11T17:34:32.003096Z","iopub.status.idle":"2022-07-11T17:34:32.029738Z","shell.execute_reply.started":"2022-07-11T17:34:32.003051Z","shell.execute_reply":"2022-07-11T17:34:32.028715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_attention(image, result, attention_plot):\n    temp_image = np.array(Image.open(image))\n\n    fig = plt.figure(figsize=(10, 10))\n\n    len_result = len(result)\n    for i in range(len_result):\n        temp_att = np.resize(attention_plot[i], (8, 8))\n        grid_size = max(int(np.ceil(len_result/2)), 2)\n        ax = fig.add_subplot(grid_size, grid_size, i+1)\n        ax.set_title(result[i])\n        img = ax.imshow(temp_image)\n        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-11T17:34:32.031592Z","iopub.execute_input":"2022-07-11T17:34:32.032118Z","iopub.status.idle":"2022-07-11T17:34:32.050617Z","shell.execute_reply.started":"2022-07-11T17:34:32.032076Z","shell.execute_reply":"2022-07-11T17:34:32.049495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# captions on the validation set\nrid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\nresult, attention_plot = evaluate(image)\n\nprint ('Real Caption:', real_caption)\nprint ('Prediction Caption:', ' '.join(result))\nplot_attention(image, result, attention_plot)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T17:34:32.053497Z","iopub.execute_input":"2022-07-11T17:34:32.053850Z","iopub.status.idle":"2022-07-11T17:34:35.139071Z","shell.execute_reply.started":"2022-07-11T17:34:32.053820Z","shell.execute_reply":"2022-07-11T17:34:35.138183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_without_plot(image):\n    hidden = decoder.reset_state(batch_size=1)\n\n    temp_input = tf.expand_dims(load_image(image)[0], 0)\n    img_tensor_val = image_features_extract_model(temp_input)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\n    features = encoder(img_tensor_val)\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n    result = []\n\n    for i in range(max_length):\n        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n        result.append(tokenizer.index_word[predicted_id])\n\n        if tokenizer.index_word[predicted_id] == '<end>':\n            return result\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-07-11T17:34:35.140710Z","iopub.execute_input":"2022-07-11T17:34:35.141136Z","iopub.status.idle":"2022-07-11T17:34:35.150618Z","shell.execute_reply.started":"2022-07-11T17:34:35.141093Z","shell.execute_reply":"2022-07-11T17:34:35.149222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_captions = []\npred_captions = []\n\nwith open('./all_captions.csv', 'w') as file:\n    writer = csv.writer(file)\n    writer.writerow([\"true_caption\", \"pred_caption\"])\n\n    for idx in tqdm(range(len(img_name_val))):\n        r_cap = [tokenizer.index_word[i] for i in cap_val[idx] if i not in [0]][1:-1]\n        p_cap = evaluate_without_plot(img_name_val[idx])[:-1]\n    \n        real_captions.append(r_cap)\n        pred_captions.append(p_cap)\n\n        writer.writerow([' '.join(r_cap), ' '.join(p_cap)])","metadata":{"execution":{"iopub.status.busy":"2022-07-11T17:34:35.152152Z","iopub.execute_input":"2022-07-11T17:34:35.153153Z","iopub.status.idle":"2022-07-11T17:38:24.196327Z","shell.execute_reply.started":"2022-07-11T17:34:35.153106Z","shell.execute_reply":"2022-07-11T17:38:24.195407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#image_url = '..input/test-image/running-dog-3770607.jpg'\n#image_extension = image_url[-4:]\n#image_path = tf.keras.utils.get_file('image'+image_extension,\n#                                     origin=image_url)\n\nimg,img_path=load_image(\"..input/test-image/running-dog-3770607.jpg\")\n\nresult, attention_plot = evaluate(img)\nprint ('Prediction Caption:', ' '.join(result))\nplot_attention(image_path, result, attention_plot)\n# opening the image\nImage.open(image_path)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T17:38:24.197933Z","iopub.execute_input":"2022-07-11T17:38:24.198902Z","iopub.status.idle":"2022-07-11T17:38:25.329671Z","shell.execute_reply.started":"2022-07-11T17:38:24.198858Z","shell.execute_reply":"2022-07-11T17:38:25.328436Z"},"trusted":true},"execution_count":null,"outputs":[]}]}